# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 16629017055157332520), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1107729785138005413)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s1000_l4_u64/hparams
  saving hparams to tmp/nmt_attention_model_DE_s1000_l4_u64/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s1000_l4_u64/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=4
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=4
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=64
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s1000_l4_u64
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), 
# log_file=tmp/nmt_attention_model_DE_s1000_l4_u64/log_1574822068
  created train model with fresh parameters, time 0.24s
  created infer model with fresh parameters, time 0.11s
  # 977
    src: Angst vor Rezession in Europa belastet US ##AT##-##AT## Börse
    ref: Fear of recession in Europe depresses US stock exchange
    nmt: weapons Voda Voda Arabella asked asked Whale Whale Whale breezes Significantly Significantly Significantly Significantly Significantly Significantly Significantly WORLD
  created eval model with fresh parameters, time 0.12s
  eval dev: perplexity 50000.27, time 29s, Tue Nov 26 23:34:58 2019.
  eval test: perplexity 50000.05, time 25s, Tue Nov 26 23:35:24 2019.
  created infer model with fresh parameters, time 0.10s
# Start step 0, lr 1, Tue Nov 26 23:35:24 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 2.10s wps 2.90K ppl 47068.02 gN 64.44 bleu 0.00, Tue Nov 26 23:38:54 2019
  step 200 lr 1 step-time 1.90s wps 3.22K ppl 6107.55 gN 28.44 bleu 0.00, Tue Nov 26 23:42:04 2019
  step 300 lr 1 step-time 1.88s wps 3.21K ppl 3546.19 gN 21.17 bleu 0.00, Tue Nov 26 23:45:12 2019
  step 400 lr 1 step-time 1.90s wps 3.21K ppl 2734.05 gN 16.81 bleu 0.00, Tue Nov 26 23:48:22 2019
  step 500 lr 1 step-time 1.91s wps 3.22K ppl 2417.77 gN 15.63 bleu 0.00, Tue Nov 26 23:51:33 2019
  step 600 lr 1 step-time 1.88s wps 3.23K ppl 1992.32 gN 12.92 bleu 0.00, Tue Nov 26 23:54:42 2019
  step 700 lr 1 step-time 1.90s wps 3.21K ppl 1846.20 gN 12.04 bleu 0.00, Tue Nov 26 23:57:52 2019
  step 800 lr 1 step-time 1.89s wps 3.21K ppl 1659.20 gN 10.73 bleu 0.00, Wed Nov 27 00:01:01 2019
  step 900 lr 1 step-time 1.89s wps 3.20K ppl 1577.73 gN 9.75 bleu 0.00, Wed Nov 27 00:04:10 2019
  step 1000 lr 1 step-time 1.90s wps 3.19K ppl 1366.98 gN 7.41 bleu 0.00, Wed Nov 27 00:07:20 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l4_u64/translate.ckpt-1000, time 0.44s
  # 1580
    src: Einige kamen in der Hoffnung , Breivik in die Augen sehen zu können , andere wollten die Bestätigung , dass er hinter Schloss und Riegel bleibt .
    ref: Some came hoping to look Breivik in the eye , others to confirm that he is being held under lock and key .
    nmt: This is is is is in in to , ,
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l4_u64/translate.ckpt-1000, time 0.06s
  eval dev: perplexity 1476.18, time 31s, Wed Nov 27 00:07:52 2019.
  eval test: perplexity 1278.87, time 25s, Wed Nov 27 00:08:18 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l4_u64/translate.ckpt-1000, time 0.43s
  # 707
    src: &quot; Die Deutsche Bank muss alle Anlagen aus ihrem Portfolio streichen , bei denen Geld für Wetten auf Nahrungsmittelpreise eingesetzt wird &quot; , sagte Bode .
    ref: &quot; Deutsche Bank must remove all investments from its portfolio that bet money on food prices , &quot; said Bode .
    nmt: This is is is is in in to , ,
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l4_u64/translate.ckpt-1000, time 0.05s
  eval dev: perplexity 1476.18, time 30s, Wed Nov 27 00:08:51 2019.
  eval test: perplexity 1278.87, time 25s, Wed Nov 27 00:09:16 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l4_u64/translate.ckpt-1000, time 0.05s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l4_u64/output_dev
  done, num sentences 3003, num translations per input 1, time 9s, Wed Nov 27 00:09:26 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l4_u64/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l4_u64/output_test
  done, num sentences 3000, num translations per input 1, time 9s, Wed Nov 27 00:09:36 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l4_u64/hparams
# Final, step 1000 lr 1 step-time 1.90s wps 3.19K ppl 1366.98 gN 7.41 dev ppl 1476.18, dev bleu 0.0, test ppl 1278.87, test bleu 0.0, Wed Nov 27 00:09:36 2019
# Done training!, time 2051s, Wed Nov 27 00:09:36 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.10s
  # 199
    src: In der Wüste zum Beispiel wird man von einem Wüstensturm überrascht , der die Sicht stark einschränkt .
    ref: For example , in the desert , you get into a sandstorm , significantly decreasing the visibility .
    nmt: Brilliant Brilliant Brilliant integer integer integer integer integer excluded excluded excluded Honeymoon Honeymoon Bristol Bristol Bristol ecclesiastical ecclesiastical ecclesiastical ecclesiastical ROOM ROOM ROOM ROOM ROOM ROOM ROOM ROOM ROOM ROOM ROOM ROOM 0049 0049 mi mi
  created eval model with fresh parameters, time 0.12s
  eval dev: perplexity 50000.44, time 29s, Wed Nov 27 00:10:06 2019.
  eval test: perplexity 50000.23, time 25s, Wed Nov 27 00:10:32 2019.
  created infer model with fresh parameters, time 0.11s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 1.90s wps 3.19K ppl 1366.98 gN 7.41 dev ppl 50000.44, dev bleu 0.0, test ppl 50000.23, test bleu 0.0, Wed Nov 27 00:10:32 2019
