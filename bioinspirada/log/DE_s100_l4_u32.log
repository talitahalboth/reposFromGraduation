# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12301385899416036254), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5936675734425430710)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s100_l4_u32/hparams
  saving hparams to tmp/nmt_attention_model_DE_s100_l4_u32/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s100_l4_u32/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=4
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=4
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=100
  num_translations_per_input=1
  num_units=32
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s100_l4_u32
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=100, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), 
# log_file=tmp/nmt_attention_model_DE_s100_l4_u32/log_1574792127
  created train model with fresh parameters, time 0.23s
  created infer model with fresh parameters, time 0.10s
  # 441
    src: Hand aufs Herz - das ist wohl allen Frauen eigen .
    ref: Let &apos;s be frank , this is probably the nature all women .
    nmt: overlook overlook Aquitaine Aquitaine Inglewood Inglewood Inglewood consenting consenting Playtech Playtech Playtech fucks multilingual multilingual multilingual multilingual multilingual CUP meantime meantime symbols
  created eval model with fresh parameters, time 0.10s
  eval dev: perplexity 50002.16, time 30s, Tue Nov 26 15:15:59 2019.
  eval test: perplexity 50002.15, time 24s, Tue Nov 26 15:16:23 2019.
  created infer model with fresh parameters, time 0.08s
# Start step 0, lr 1, Tue Nov 26 15:16:24 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 1.88s wps 3.24K ppl 41110.89 gN 54.62 bleu 0.00, Tue Nov 26 15:19:32 2019
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l4_u32/translate.ckpt-100, time 0.26s
  # 2405
    src: Das BIP w채chst um 1,5 % dank des Inlandskonsums und beendet die Spirale des freien Falls ab Anfang M채rz
    ref: GDP grows by 1.5 % due to domestic consumption and puts an end to the downward spiral that began in March
    nmt: The <unk> . . . . .
  loaded eval model parameters from tmp/nmt_attention_model_DE_s100_l4_u32/translate.ckpt-100, time 0.06s
  eval dev: perplexity 111782.90, time 29s, Tue Nov 26 15:20:02 2019.
  eval test: perplexity 90527.52, time 24s, Tue Nov 26 15:20:27 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l4_u32/translate.ckpt-100, time 0.04s
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l4_u32/output_dev
  done, num sentences 3003, num translations per input 1, time 4s, Tue Nov 26 15:20:32 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l4_u32/hparams
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l4_u32/output_test
  done, num sentences 3000, num translations per input 1, time 4s, Tue Nov 26 15:20:37 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l4_u32/hparams
# Final, step 100 lr 1 step-time 1.88s wps 3.24K ppl 41110.89 gN 54.62 dev ppl 111782.90, dev bleu 0.0, test ppl 90527.52, test bleu 0.0, Tue Nov 26 15:20:37 2019
# Done training!, time 253s, Tue Nov 26 15:20:37 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.09s
  # 2931
    src: Moleskine profitierte von seinem Erfolg und diesem kulturellen Zeitgeist und verstand es , seine Produkte zu diversifizieren : Taschen , Terminkalender , F체ller , Computerh체llen ... Die Marke weist die gesamte Bandbreite von Artikeln auf .
    ref: Exploiting its success and the cultural aura , Moleskine diversified its products to include handbags , pens , computer bags . The brand includes every kind of support .
    nmt: unpacked animations animations animations admitted hobbyists recessive Annexes Annexes Annexes Annexes 12.30 12.30 device evine evine evine evine evine evine Avril Avril Avril 12.30 12.30 12.30 12.30 12.30 Y Y disconnect disconnect disconnect disconnect disconnect disconnect evine topple topple topple topple topple topple topple topple topple destabilisation KStars KStars KStars KStars KStars KStars KStars KStars KStars KStars Break Ghost Ghost wrist wrist wrist wrist wrist wrist chartering Runtime Runtime toned toned toned
  created eval model with fresh parameters, time 0.10s
  eval dev: perplexity 50002.11, time 29s, Tue Nov 26 15:21:07 2019.
  eval test: perplexity 50002.05, time 24s, Tue Nov 26 15:21:32 2019.
  created infer model with fresh parameters, time 0.09s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 1.88s wps 3.24K ppl 41110.89 gN 54.62 dev ppl 50002.11, dev bleu 0.0, test ppl 50002.05, test bleu 0.0, Tue Nov 26 15:21:32 2019
