# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 11028428125862389322), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8160995526231054312)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u256/hparams
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u256/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s1000_l2_u256/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=256
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s1000_l2_u256
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), 
# log_file=tmp/nmt_attention_model_DE_s1000_l2_u256/log_1574815621
  created train model with fresh parameters, time 0.25s
  created infer model with fresh parameters, time 0.17s
  # 1683
    src: Boeing zieht Rekordauftrag von 18 Billionen Dollar an Land
    ref: Boeing gets record $ 18 ##AT##-##AT## billion jetliner order
    nmt: supplementing supplementing gazebo Recommendations sound 10 sound sound sound Duque Duque Duque Famara Famara Famara Famara Famara Famara
  created eval model with fresh parameters, time 0.17s
  eval dev: perplexity 50321.55, time 45s, Tue Nov 26 21:47:47 2019.
  eval test: perplexity 50308.59, time 38s, Tue Nov 26 21:48:25 2019.
  created infer model with fresh parameters, time 0.17s
# Start step 0, lr 1, Tue Nov 26 21:48:26 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 3.89s wps 1.57K ppl 145433.35 gN 134.79 bleu 0.00, Tue Nov 26 21:54:54 2019
  step 200 lr 1 step-time 3.65s wps 1.67K ppl 7992.41 gN 41.84 bleu 0.00, Tue Nov 26 22:00:59 2019
  step 300 lr 1 step-time 3.65s wps 1.67K ppl 3231.53 gN 27.33 bleu 0.00, Tue Nov 26 22:07:04 2019
  step 400 lr 1 step-time 3.67s wps 1.67K ppl 1979.39 gN 17.24 bleu 0.00, Tue Nov 26 22:13:11 2019
  step 500 lr 1 step-time 3.64s wps 1.67K ppl 1532.32 gN 15.55 bleu 0.00, Tue Nov 26 22:19:15 2019
  step 600 lr 1 step-time 3.64s wps 1.67K ppl 1263.90 gN 12.44 bleu 0.00, Tue Nov 26 22:25:19 2019
  step 700 lr 1 step-time 3.62s wps 1.67K ppl 1034.75 gN 8.88 bleu 0.00, Tue Nov 26 22:31:20 2019
  step 800 lr 1 step-time 3.67s wps 1.68K ppl 914.32 gN 8.44 bleu 0.00, Tue Nov 26 22:37:27 2019
  step 900 lr 1 step-time 3.64s wps 1.67K ppl 820.72 gN 7.61 bleu 0.00, Tue Nov 26 22:43:31 2019
  step 1000 lr 1 step-time 3.63s wps 1.67K ppl 683.40 gN 6.92 bleu 0.00, Tue Nov 26 22:49:33 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u256/translate.ckpt-1000, time 1.60s
  # 68
    src: Mit dieser Geste wollten sie ihre Unterstützung für das Kernprogramm ihres Landes vor einem möglichen israelischen Angriff demonstrieren .
    ref: The gesture is to express their support of their country &apos;s nuclear program against a possible Israel attack .
    nmt: In the hotel of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u256/translate.ckpt-1000, time 0.14s
  eval dev: perplexity 977.95, time 46s, Tue Nov 26 22:50:25 2019.
  eval test: perplexity 880.41, time 38s, Tue Nov 26 22:51:04 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u256/translate.ckpt-1000, time 1.61s
  # 1456
    src: Die Öffentlichkeit benimmt sich zunehmend so , als täten ihr die Menschen in Uniform Leid .
    ref: The public increasingly acts as if it feels sorry for those in uniform .
    nmt: The hotel of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u256/translate.ckpt-1000, time 0.13s
  eval dev: perplexity 977.95, time 45s, Tue Nov 26 22:51:54 2019.
  eval test: perplexity 880.41, time 38s, Tue Nov 26 22:52:33 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u256/translate.ckpt-1000, time 0.13s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u256/output_dev
  done, num sentences 3003, num translations per input 1, time 228s, Tue Nov 26 22:56:21 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u256/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u256/output_test
  done, num sentences 3000, num translations per input 1, time 208s, Tue Nov 26 22:59:51 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u256/hparams
# Final, step 1000 lr 1 step-time 3.63s wps 1.67K ppl 683.40 gN 6.92 dev ppl 977.95, dev bleu 0.0, test ppl 880.41, test bleu 0.0, Tue Nov 26 22:59:52 2019
# Done training!, time 4286s, Tue Nov 26 22:59:52 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.17s
  # 424
    src: Ordnung und Sauberkeit an erster Stelle
    ref: Order and Cleanliness First
    nmt: PARTNER PARTNER PARTNER że że że Lawyer Lawyer EURO immune 27th 27th
  created eval model with fresh parameters, time 0.19s
  eval dev: perplexity 49896.19, time 46s, Tue Nov 26 23:00:39 2019.
  eval test: perplexity 49911.93, time 38s, Tue Nov 26 23:01:18 2019.
  created infer model with fresh parameters, time 0.19s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 3.63s wps 1.67K ppl 683.40 gN 6.92 dev ppl 49896.19, dev bleu 0.0, test ppl 49911.93, test bleu 0.0, Tue Nov 26 23:01:20 2019
