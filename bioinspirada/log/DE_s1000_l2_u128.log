# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 2806968039562768979), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14589198354793062461)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u128/hparams
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u128/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s1000_l2_u128/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=128
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s1000_l2_u128
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50000), 
# log_file=tmp/nmt_attention_model_DE_s1000_l2_u128/log_1574812849
  created train model with fresh parameters, time 0.20s
  created infer model with fresh parameters, time 0.12s
  # 2317
    src: Der erste Faktor , der als entscheidend gilt für den Beschluss der Jury , sie zur Ersten Miss zu ernennen , ist , dass sie die Antwort auf die Frage &quot; Welches Buch hat dich am stärksten beeinflusst und warum ? &quot; , die ihr von Martín Murillo Gómez von der Carreta Literaria Leamos gestellt worden war , nicht überzeugt hatte .
    ref: The first key factor in the decision of the jury to designate her as First Princess is that she did not convince with her answer to the question &quot; &apos; What is the book that has marked you and why ? Asked ​ ​ by Martin Murillo Gomez from the Literary Cart Let &apos;s read .
    nmt: apace apace Bamberg Bamberg 528 rearmament rearmament 710 710 Trading Trading Guild remarkable remarkable remarkable remarkable vulgar Atlantis Atlantis Atlantis Atlantis playback playback playback plugging plugging maceration Guild Guild maceration maceration Masaya Masaya Masaya spend spend spend spend spend plateau plateau outros emperors emperors emperors Finding Finding Finding scenarios scenarios scenarios scenarios conglomerates conglomerates conglomerates conglomerates glider glider glider glider glider glider glider shatter shatter shatter shatter Stazione Stazione Dutch Stazione Stazione Awareness Awareness Vibration Goldman Goldman Goldman Goldman Goldman Goldman revives revives revives revives revives revives writ writ writ Tuscany Tuscany SDSS SDSS Tuscany Tuscany SDSS Faroe Faroe Faroe Faroe trolls interested positively positively positively positively positively beings polling polling beings polling beings beings polling archipelagos archipelagos Nou Nou Nou Nou Nou strategist
  created eval model with fresh parameters, time 0.13s
  eval dev: perplexity 50008.72, time 33s, Tue Nov 26 21:01:24 2019.
  eval test: perplexity 50012.76, time 28s, Tue Nov 26 21:01:52 2019.
  created infer model with fresh parameters, time 0.10s
# Start step 0, lr 1, Tue Nov 26 21:01:53 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 2.53s wps 2.42K ppl 78560.81 gN 102.07 bleu 0.00, Tue Nov 26 21:06:06 2019
  step 200 lr 1 step-time 2.32s wps 2.65K ppl 4690.42 gN 28.38 bleu 0.00, Tue Nov 26 21:09:58 2019
  step 300 lr 1 step-time 2.28s wps 2.65K ppl 1886.03 gN 13.16 bleu 0.00, Tue Nov 26 21:13:45 2019
  step 400 lr 1 step-time 2.31s wps 2.66K ppl 1432.00 gN 10.18 bleu 0.00, Tue Nov 26 21:17:37 2019
  step 500 lr 1 step-time 2.27s wps 2.65K ppl 1109.24 gN 6.95 bleu 0.00, Tue Nov 26 21:21:24 2019
  step 600 lr 1 step-time 2.28s wps 2.65K ppl 970.83 gN 7.13 bleu 0.00, Tue Nov 26 21:25:12 2019
  step 700 lr 1 step-time 2.29s wps 2.65K ppl 921.48 gN 7.71 bleu 0.00, Tue Nov 26 21:29:01 2019
  step 800 lr 1 step-time 2.29s wps 2.65K ppl 805.00 gN 7.01 bleu 0.00, Tue Nov 26 21:32:50 2019
  step 900 lr 1 step-time 2.29s wps 2.65K ppl 733.92 gN 5.92 bleu 0.00, Tue Nov 26 21:36:39 2019
  step 1000 lr 1 step-time 2.29s wps 2.64K ppl 663.73 gN 5.79 bleu 0.00, Tue Nov 26 21:40:28 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u128/translate.ckpt-1000, time 0.81s
  # 807
    src: &quot; Sie haben einen tollen Job gemacht &quot; , sagte Buffett zur Strategie .
    ref: &quot; They have done a great job , &quot; said Buffett , referring to the management strategy .
    nmt: The hotel is a hotel of <unk> , , , , , , .
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u128/translate.ckpt-1000, time 0.07s
  eval dev: perplexity 1046.40, time 33s, Tue Nov 26 21:41:04 2019.
  eval test: perplexity 906.59, time 28s, Tue Nov 26 21:41:33 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u128/translate.ckpt-1000, time 0.81s
  # 2529
    src: Vom gesamten Land her betrachtet ist die Rate mit 86 % an im Krankenhaus eingetretenen Sterbefällen im Quebec am höchsten .
    ref: Countrywide , it is in Quebec that the rate is highest , with 86 % of deaths taking place in hospital .
    nmt: The hotel is a hotel of , , , , , , , , , , , , .
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u128/translate.ckpt-1000, time 0.07s
  eval dev: perplexity 1046.40, time 34s, Tue Nov 26 21:42:11 2019.
  eval test: perplexity 906.59, time 28s, Tue Nov 26 21:42:40 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u128/translate.ckpt-1000, time 0.06s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u128/output_dev
  done, num sentences 3003, num translations per input 1, time 97s, Tue Nov 26 21:44:18 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u128/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u128/output_test
  done, num sentences 3000, num translations per input 1, time 79s, Tue Nov 26 21:45:38 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u128/hparams
# Final, step 1000 lr 1 step-time 2.29s wps 2.64K ppl 663.73 gN 5.79 dev ppl 1046.40, dev bleu 0.0, test ppl 906.59, test bleu 0.0, Tue Nov 26 21:45:38 2019
# Done training!, time 2625s, Tue Nov 26 21:45:38 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.11s
  # 2621
    src: Österreich
    ref: Austria
    nmt: Elk Elk
  created eval model with fresh parameters, time 0.12s
  eval dev: perplexity 49992.83, time 34s, Tue Nov 26 21:46:14 2019.
  eval test: perplexity 49996.99, time 28s, Tue Nov 26 21:46:42 2019.
  created infer model with fresh parameters, time 0.11s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 2.29s wps 2.64K ppl 663.73 gN 5.79 dev ppl 49992.83, dev bleu 0.0, test ppl 49996.99, test bleu 0.0, Tue Nov 26 21:46:43 2019
