# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 5363420766007405962), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14705146886949813109)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u64/hparams
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u64/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s1000_l2_u64/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=64
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s1000_l2_u64
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), 
# log_file=tmp/nmt_attention_model_DE_s1000_l2_u64/log_1574810613
  created train model with fresh parameters, time 0.16s
  created infer model with fresh parameters, time 0.10s
  # 594
    src: &quot; Sollte ich Vorsitzender werden , dann nehme ich mich dessen persönlich an und bin bereit persönlich Verantwortung zu tragen , wenn es nicht klappt . &quot;
    ref: &quot; If I become President , I will take it as my personal task , and I &apos;m willing to take personal responsibility in case the attempt fails . &quot;
    nmt: tigers fieldbus reaches reaches Maier ENT ENT lawsuit subsidize subsidize subsidize Nou Nou timestamps Nou adhered adhered adhered adhered READ READ READ Gianfranco ushers rename rename rename rename rename edible edible edible TM TM TM OU OU motivation HOTELS UX Manchu Manchu grandeur grandeur inappropriate group ± Insofar ± Insofar Bing accountability accountability READ
  created eval model with fresh parameters, time 0.10s
  eval dev: perplexity 50002.55, time 29s, Tue Nov 26 20:24:03 2019.
  eval test: perplexity 50002.12, time 25s, Tue Nov 26 20:24:28 2019.
  created infer model with fresh parameters, time 0.08s
# Start step 0, lr 1, Tue Nov 26 20:24:28 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 2.03s wps 3.00K ppl 43787.51 gN 75.14 bleu 0.00, Tue Nov 26 20:27:51 2019
  step 200 lr 1 step-time 1.84s wps 3.35K ppl 2748.01 gN 16.42 bleu 0.00, Tue Nov 26 20:30:55 2019
  step 300 lr 1 step-time 1.82s wps 3.35K ppl 1646.65 gN 9.70 bleu 0.00, Tue Nov 26 20:33:57 2019
  step 400 lr 1 step-time 1.83s wps 3.34K ppl 1298.43 gN 8.76 bleu 0.00, Tue Nov 26 20:37:00 2019
  step 500 lr 1 step-time 1.82s wps 3.33K ppl 1091.13 gN 7.55 bleu 0.00, Tue Nov 26 20:40:03 2019
  step 600 lr 1 step-time 1.81s wps 3.34K ppl 952.82 gN 6.93 bleu 0.00, Tue Nov 26 20:43:04 2019
  step 700 lr 1 step-time 1.82s wps 3.33K ppl 851.54 gN 7.06 bleu 0.00, Tue Nov 26 20:46:06 2019
  step 800 lr 1 step-time 1.81s wps 3.35K ppl 751.92 gN 5.97 bleu 0.00, Tue Nov 26 20:49:06 2019
  step 900 lr 1 step-time 1.82s wps 3.33K ppl 699.20 gN 5.98 bleu 0.00, Tue Nov 26 20:52:08 2019
  step 1000 lr 1 step-time 1.81s wps 3.34K ppl 642.57 gN 5.29 bleu 0.00, Tue Nov 26 20:55:09 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u64/translate.ckpt-1000, time 0.43s
  # 2057
    src: Wir werden versuchen , die bestmögliche Show zu geben mit vielen verschiedenen Liedern . Wir wollen ein paar Songs aus jedem Album spielen und unseren Fans ein gutes und vielseitiges Repertoire bieten .
    ref: We will try to make the best show possible with a good variety of songs ; we will try to play a couple of songs of each album and give our fans a good and diverse repertoire .
    nmt: The hotel of the <unk> of the <unk> of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u64/translate.ckpt-1000, time 0.05s
  eval dev: perplexity 827.54, time 30s, Tue Nov 26 20:55:41 2019.
  eval test: perplexity 750.03, time 25s, Tue Nov 26 20:56:07 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u64/translate.ckpt-1000, time 0.42s
  # 743
    src: Beim jetzigen Umbau gab es weit größere Probleme : Das Gebäude war &quot; eine Energieschleuder &quot; , sagt Krischanitz .
    ref: The problems facing the latest alterations were much greater . The building was &quot; an energy guzzler , &quot; says Krischanitz .
    nmt: The hotel of the <unk> of the <unk> of the city of the city of the city of the city of the city of the city of the city of the city of the city of the city of the
  loaded eval model parameters from tmp/nmt_attention_model_DE_s1000_l2_u64/translate.ckpt-1000, time 0.05s
  eval dev: perplexity 827.54, time 29s, Tue Nov 26 20:56:38 2019.
  eval test: perplexity 750.03, time 25s, Tue Nov 26 20:57:03 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s1000_l2_u64/translate.ckpt-1000, time 0.04s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u64/output_dev
  done, num sentences 3003, num translations per input 1, time 77s, Tue Nov 26 20:58:20 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u64/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_DE_s1000_l2_u64/output_test
  done, num sentences 3000, num translations per input 1, time 70s, Tue Nov 26 20:59:32 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s1000_l2_u64/hparams
# Final, step 1000 lr 1 step-time 1.81s wps 3.34K ppl 642.57 gN 5.29 dev ppl 827.54, dev bleu 0.0, test ppl 750.03, test bleu 0.0, Tue Nov 26 20:59:33 2019
# Done training!, time 2104s, Tue Nov 26 20:59:33 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.08s
  # 1276
    src: Stellen Sie sich vor , Sie sind eine Frau , die mit ihren Kindern zu Hause ist , und Kanonenfeuer in der Ferne hört . Bleiben Sie oder ergreifen Sie die Flucht ?
    ref: When cannon fire is heard in the distance , and you are a woman at home with your children , do you stay or flee ?
    nmt: judge judge add chromosomes chromosomes Springsteen Springsteen Yeah Unknown circulate circulate Hot Hot Hot Ones quaint TeX TeX prophecy prophecy overloaded overloaded CONTENT changeable changeable changeable changeable CONTENT changeable changeable seems seems telescopes telescopes telescopes telescopes railways repress Tutsi Tutsi renunciation Asylum journalists journalists Pasqua Pasqua Pasqua Boulogne Boulogne Reform Corralejo Corralejo electron electron telecom telecom Karimov trembling trembling unnecessary unnecessary Koenig ventured Mercosur Mercosur stipulate
  created eval model with fresh parameters, time 0.09s
  eval dev: perplexity 49996.47, time 30s, Tue Nov 26 21:00:03 2019.
  eval test: perplexity 49996.53, time 25s, Tue Nov 26 21:00:29 2019.
  created infer model with fresh parameters, time 0.08s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 1.81s wps 3.34K ppl 642.57 gN 5.29 dev ppl 49996.47, dev bleu 0.0, test ppl 49996.53, test bleu 0.0, Tue Nov 26 21:00:30 2019
