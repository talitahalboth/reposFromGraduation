# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 13921603647447059968), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8836023415549316144)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u64/hparams
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u64/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s100_l2_u64/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=100
  num_translations_per_input=1
  num_units=64
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s100_l2_u64
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=100, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 64), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 64), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (64, 64), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (192, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (128, 256), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (256,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (128, 64), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (64, 50000), 
# log_file=tmp/nmt_attention_model_DE_s100_l2_u64/log_1574790002
  created train model with fresh parameters, time 0.17s
  created infer model with fresh parameters, time 0.09s
  # 296
    src: Wie soll ich als Bürger eigentlich wissen , ob man sich in Brüssel wirklich krumme oder runde Gurken und verschieden große Bananen ausdenkt oder ob uns tatsächlich jemand die Quargel verbieten will .
    ref: How are citizens supposed to understand that in Brussels , people are not trying to come up with crooked or round cucumbers , different size bananas , or that someone wants to ban our cheese specialties.&apos;
    nmt: Sulu Lax Wilderness Wilderness Wilderness Wilderness Wilderness Wilderness Buying agua agua Born Born Born Born exposition exposition &apos;en exposition henchmen Burns suitable ideally inestimable register register titular Lam Lam Jaguar Giuseppe Giuseppe beers beers Organizer Organizer Organizer Organizer criticizes criticizes criticizes criticizes criticizes mammals HOT HOT HOT Andy Andy swept swept employee Lecture Lecture composition peaceful Pair Pair grammatically contesting leather contesting whisper whisper summed summed
  created eval model with fresh parameters, time 0.10s
  eval dev: perplexity 49999.32, time 29s, Tue Nov 26 14:40:32 2019.
  eval test: perplexity 49999.73, time 25s, Tue Nov 26 14:40:57 2019.
  created infer model with fresh parameters, time 0.08s
# Start step 0, lr 1, Tue Nov 26 14:40:57 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 2.03s wps 3.00K ppl 83529.00 gN 79.18 bleu 0.00, Tue Nov 26 14:44:20 2019
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l2_u64/translate.ckpt-100, time 0.42s
  # 1070
    src: Montenegro hatte seinen größten Möglichkeiten , wenn das Duo Stevan Jovetic und Dejan Damjanovic in Aktion war .
    ref: Montenegro had its best opportunities when the Stevan Jovetic and Dejan Damjanovic duo were on the field .
    nmt: The the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
  loaded eval model parameters from tmp/nmt_attention_model_DE_s100_l2_u64/translate.ckpt-100, time 0.05s
  eval dev: perplexity 3445.65, time 30s, Tue Nov 26 14:44:52 2019.
  eval test: perplexity 3372.33, time 25s, Tue Nov 26 14:45:17 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l2_u64/translate.ckpt-100, time 0.05s
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l2_u64/output_dev
  done, num sentences 3003, num translations per input 1, time 77s, Tue Nov 26 14:46:35 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u64/hparams
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l2_u64/output_test
  done, num sentences 3000, num translations per input 1, time 70s, Tue Nov 26 14:47:46 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u64/hparams
# Final, step 100 lr 1 step-time 2.03s wps 3.00K ppl 83529.00 gN 79.18 dev ppl 3445.65, dev bleu 0.0, test ppl 3372.33, test bleu 0.0, Tue Nov 26 14:47:46 2019
# Done training!, time 409s, Tue Nov 26 14:47:46 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.09s
  # 1326
    src: Herr Kelly sagte , dass im Laufe des Tages viele Personen im Park ein- und ausgehen , fast wie Pendler , so dass 1 Uhr in der Nacht ein guter Zeitpunkt ist , um hineinzugehen .
    ref: Mr. Kelly said many people , almost like commuters , had been coming and going from the park during the day , making 1 a.m. a good time to move in .
    nmt: inOffice evading Jockey ensuring ensuring Alberto Alberto Low Low Ripper Ripper Ripper Ripper hacked hacked knowingly knowingly knowingly knowingly knowingly 368 LEADER LEADER LEADER FOCUS Fire Fire Fire fought fought Articles Articles ahead meander cleft cleft cleft Postal Postal Own sway sway sway sway Agricola Mina Mina pas dressing Le exhort exhort Frau Frau Frau Frau non non non non bem bem Assisi inOffice Tuning Tuning Tuning Tuning aluminum SCA SCA SCA
  created eval model with fresh parameters, time 0.09s
  eval dev: perplexity 49999.19, time 29s, Tue Nov 26 14:48:16 2019.
  eval test: perplexity 50000.44, time 25s, Tue Nov 26 14:48:41 2019.
  created infer model with fresh parameters, time 0.09s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 2.03s wps 3.00K ppl 83529.00 gN 79.18 dev ppl 49999.19, dev bleu 0.0, test ppl 50000.44, test bleu 0.0, Tue Nov 26 14:48:43 2019
