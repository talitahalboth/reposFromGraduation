# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 3739309156196722514), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18406852750595089786)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s500_l4_u256/hparams
  saving hparams to tmp/nmt_attention_model_DE_s500_l4_u256/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s500_l4_u256/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=4
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=4
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=500
  num_translations_per_input=1
  num_units=256
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s500_l4_u256
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=500, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 50000), 
# log_file=tmp/nmt_attention_model_DE_s500_l4_u256/log_1574805686
  created train model with fresh parameters, time 0.33s
  created infer model with fresh parameters, time 0.19s
  # 2347
    src: 2000 siedelte er nach Valle über und nach der Vollendung der erforderlichen Kurse spezialisierte er sich auf Lophoskopie , das ist die Untersuchung der Haut , die Reibungen ausgesetzt ist , d.h. der Hand- und Fußflächen .
    ref: In 2000 he moved to the capital of the Valley and , after making the corresponding courses , he specialized in lophoscopy , which is the study of skin friction , meaning the palms of the hands and feet .
    nmt: Implant domino domino crept crept crept competes competes competes competes competes competes leash Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians Phoenicians 94 94 Zhou Zhou Zhou Zhou Zhou Zhou Zhou Zhou Zhou Zhou Zhou baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone baritone Widget Widget Widget Widget Widget Widget canton canton canton canton canton canton canton canton Ventura Ventura Ventura referrer referrer
  created eval model with fresh parameters, time 0.19s
  eval dev: perplexity 50178.18, time 49s, Tue Nov 26 19:02:18 2019.
  eval test: perplexity 50125.24, time 42s, Tue Nov 26 19:03:00 2019.
  created infer model with fresh parameters, time 0.19s
# Start step 0, lr 1, Tue Nov 26 19:03:00 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 4.41s wps 1.38K ppl 1436768.14 gN 276.10 bleu 0.00, Tue Nov 26 19:10:21 2019
  step 200 lr 1 step-time 4.23s wps 1.46K ppl 42626.94 gN 84.25 bleu 0.00, Tue Nov 26 19:17:25 2019
  step 300 lr 1 step-time 4.13s wps 1.45K ppl 14134.35 gN 98.71 bleu 0.00, Tue Nov 26 19:24:18 2019
  step 400 lr 1 step-time 4.22s wps 1.45K ppl 11291.04 gN 63.15 bleu 0.00, Tue Nov 26 19:31:20 2019
  step 500 lr 1 step-time 4.15s wps 1.46K ppl 7095.21 gN 85.24 bleu 0.00, Tue Nov 26 19:38:15 2019
  loaded infer model parameters from tmp/nmt_attention_model_DE_s500_l4_u256/translate.ckpt-500, time 1.68s
  # 616
    src: &quot; Als Vorsitzender möchte ich mich dem Fußball als Ganzem widmen . &quot;
    ref: &quot; As President , I &apos;d like to focus on football as a whole . &quot;
    nmt: The <unk> <unk> <unk> <unk> <unk> <unk> , , , , , , , , , , , , , , , , , , ,
  loaded eval model parameters from tmp/nmt_attention_model_DE_s500_l4_u256/translate.ckpt-500, time 0.15s
  eval dev: perplexity 2397.81, time 54s, Tue Nov 26 19:39:15 2019.
  eval test: perplexity 2164.15, time 43s, Tue Nov 26 19:39:58 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s500_l4_u256/translate.ckpt-500, time 1.73s
# External evaluation, global step 500
  decoding to output tmp/nmt_attention_model_DE_s500_l4_u256/output_dev
  done, num sentences 3003, num translations per input 1, time 245s, Tue Nov 26 19:44:05 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s500_l4_u256/hparams
# External evaluation, global step 500
  decoding to output tmp/nmt_attention_model_DE_s500_l4_u256/output_test
  done, num sentences 3000, num translations per input 1, time 224s, Tue Nov 26 19:47:50 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s500_l4_u256/hparams
# Final, step 500 lr 1 step-time 4.15s wps 1.46K ppl 7095.21 gN 85.24 dev ppl 2397.81, dev bleu 0.0, test ppl 2164.15, test bleu 0.0, Tue Nov 26 19:47:51 2019
# Done training!, time 2690s, Tue Nov 26 19:47:51 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.20s
  # 2743
    src: Manchen hatten wohl geglaubt , es reiche aus , das Stimmrecht auszuüben , und zwar auf direktest mögliche Art und Weise , um die Demokratie ein für alle Mal über ihre Feinde obsiegen zu sehen .
    ref: Some believed that it was enough for democracy to finally triumph over its enemies , to exercise the vote , and to do so as directly as possible .
    nmt: deren aka torpedo torpedo photovoltaic photovoltaic conferring conferring conferring conferring conferring relaunched relaunched relaunched relaunched Herrmann Herrmann Herrmann Herrmann relation relation relation relation relation relation Bananas Bananas Bananas Bananas Bananas Kittelmann Kittelmann Kittelmann Kittelmann Kittelmann Kittelmann Kittelmann Markets Markets Markets Markets Markets Markets Markets Markets Markets Markets Markets Markets Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated Abbreviated quo quo quo quo quo quo quo quo Survey Survey Survey
  created eval model with fresh parameters, time 0.22s
  eval dev: perplexity 50070.07, time 49s, Tue Nov 26 19:48:42 2019.
  eval test: perplexity 50035.47, time 42s, Tue Nov 26 19:49:24 2019.
  created infer model with fresh parameters, time 0.21s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 4.15s wps 1.46K ppl 7095.21 gN 85.24 dev ppl 50070.07, dev bleu 0.0, test ppl 50035.47, test bleu 0.0, Tue Nov 26 19:49:26 2019
