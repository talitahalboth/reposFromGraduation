# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 16992234439148060738), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10007438434359454812)]
# Vocab file tmp/iwslt15/vocab.vi exists
# Vocab file tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/hparams
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_VI_s1000_l2_u256/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=tmp/iwslt15/tst2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=256
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_VI_s1000_l2_u256
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=vi
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=tmp/iwslt15/vocab.vi
  src_vocab_size=7709
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=tmp/iwslt15/tst2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=tmp/iwslt15/vocab.en
  tgt_vocab_size=17191
  time_major=True
  train_prefix=tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), 
# log_file=tmp/nmt_attention_model_VI_s1000_l2_u256/log_1574891184
  created train model with fresh parameters, time 0.16s
  created infer model with fresh parameters, time 0.10s
  # 620
    src: Sự phân chia cần phải nói lên điều gì đó tới người chọn chứ không phải là người thực hiện sự lựa chọn
    ref: The categories need to say something to the chooser , not the choice-maker .
    nmt: capacity wheels definition alliances alliances alliances ballroom alliances acrylic fierce fierce fierce fierce fierce ninja ninja ninja hybrid hybrid hybrid hybrid hybrid salute barrel barrel barrel barrel barrel barrel barrel wider wider wider wider wider wider wider wider wider wider wider wise wise wise wise wise
  created eval model with fresh parameters, time 0.09s
  eval dev: perplexity 17226.67, time 8s, Wed Nov 27 18:46:33 2019.
  eval test: perplexity 17239.63, time 8s, Wed Nov 27 18:46:42 2019.
  created infer model with fresh parameters, time 0.08s
# Start step 0, lr 1, Wed Nov 27 18:46:42 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 1.67s wps 3.33K ppl 54553.03 gN 103.88 bleu 0.00, Wed Nov 27 18:49:29 2019
  step 200 lr 1 step-time 1.62s wps 3.49K ppl 2731.16 gN 37.85 bleu 0.00, Wed Nov 27 18:52:11 2019
  step 300 lr 1 step-time 1.61s wps 3.47K ppl 933.26 gN 13.67 bleu 0.00, Wed Nov 27 18:54:52 2019
  step 400 lr 1 step-time 1.64s wps 3.45K ppl 553.16 gN 7.12 bleu 0.00, Wed Nov 27 18:57:37 2019
  step 500 lr 1 step-time 1.64s wps 3.42K ppl 459.07 gN 6.72 bleu 0.00, Wed Nov 27 19:00:20 2019
  step 600 lr 1 step-time 1.66s wps 3.43K ppl 352.52 gN 5.39 bleu 0.00, Wed Nov 27 19:03:07 2019
  step 700 lr 1 step-time 1.63s wps 3.42K ppl 287.28 gN 5.20 bleu 0.00, Wed Nov 27 19:05:50 2019
  step 800 lr 1 step-time 1.62s wps 3.49K ppl 242.19 gN 4.61 bleu 0.00, Wed Nov 27 19:08:32 2019
  step 900 lr 1 step-time 1.59s wps 3.47K ppl 215.27 gN 4.48 bleu 0.00, Wed Nov 27 19:11:11 2019
  step 1000 lr 1 step-time 1.61s wps 3.48K ppl 196.29 gN 4.29 bleu 0.00, Wed Nov 27 19:13:52 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/translate.ckpt-1000, time 4.64s
  # 958
    src: Tôi đã minh hoạ sách từ năm 16 tuổi .
    ref: I &apos;ve been illustrating books since I was 16 .
    nmt: I think I think it &apos;s a lot of the way .
  loaded eval model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/translate.ckpt-1000, time 0.06s
  eval dev: perplexity 136.67, time 8s, Wed Nov 27 19:14:11 2019.
  eval test: perplexity 155.86, time 8s, Wed Nov 27 19:14:19 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/translate.ckpt-1000, time 4.67s
  # 344
    src: thế thì mình nên xin họ một chút .
    ref: You should ask for some of that .
    nmt: So this is the <unk> .
  loaded eval model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/translate.ckpt-1000, time 0.05s
  eval dev: perplexity 136.67, time 8s, Wed Nov 27 19:14:38 2019.
  eval test: perplexity 155.86, time 8s, Wed Nov 27 19:14:47 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/translate.ckpt-1000, time 0.05s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l2_u256/output_dev
  done, num sentences 1553, num translations per input 1, time 26s, Wed Nov 27 19:15:13 2019.
  bleu dev: 0.5
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l2_u256/output_test
  done, num sentences 1268, num translations per input 1, time 24s, Wed Nov 27 19:15:43 2019.
  bleu test: 0.5
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/hparams
# Final, step 1000 lr 1 step-time 1.61s wps 3.48K ppl 196.29 gN 4.29 dev ppl 136.67, dev bleu 0.5, test ppl 155.86, test bleu 0.5, Wed Nov 27 19:15:43 2019
# Done training!, time 1741s, Wed Nov 27 19:15:43 2019.
# Start evaluating saved best models.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/best_bleu/translate.ckpt-1000, time 4.65s
  # 303
    src: có một lần tôi đến và nghe họ nói sau vài tiếng , bà Park quay sang và nói : &quot; Nào , Bryan , hãy nói cho tôi nghe về đơn kiến nghị tính công bằng của tư pháp
    ref: And one time I was over there listening to these women talk , and after a couple of hours Ms. Parks turned to me and she said , &quot; Now Bryan , tell me what the Equal Justice Initiative is .
    nmt: So I think that &apos;s the little way that &apos;s the little way that &apos;s the <unk> of the way , and I think that &apos;s the <unk> of the <unk> .
  loaded eval model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/best_bleu/translate.ckpt-1000, time 0.05s
  eval dev: perplexity 136.67, time 8s, Wed Nov 27 19:15:57 2019.
  eval test: perplexity 155.86, time 8s, Wed Nov 27 19:16:05 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l2_u256/best_bleu/translate.ckpt-1000, time 0.05s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l2_u256/output_dev
  done, num sentences 1553, num translations per input 1, time 26s, Wed Nov 27 19:16:32 2019.
  bleu dev: 0.5
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l2_u256/output_test
  done, num sentences 1268, num translations per input 1, time 24s, Wed Nov 27 19:16:57 2019.
  bleu test: 0.5
  saving hparams to tmp/nmt_attention_model_VI_s1000_l2_u256/hparams
# Best bleu, step 1000 lr 1 step-time 1.61s wps 3.48K ppl 196.29 gN 4.29 dev ppl 136.67, dev bleu 0.5, test ppl 155.86, test bleu 0.5, Wed Nov 27 19:16:57 2019
