# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 3011015809353956362), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6050369266826662906)]
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.de exists
# Vocab file /home/bcc/thcf16/tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u32/hparams
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u32/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_DE_s100_l2_u32/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=100
  num_translations_per_input=1
  num_units=32
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_DE_s100_l2_u32
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=de
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.de
  src_vocab_size=50000
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/bcc/thcf16/tmp/iwslt15/newstest2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/bcc/thcf16/tmp/iwslt15/vocab.en
  tgt_vocab_size=50000
  time_major=True
  train_prefix=/home/bcc/thcf16/tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/bcc/thcf16/tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=100, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (50000, 32), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (50000, 32), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (32, 32), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (96, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (64, 128), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (128,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (64, 32), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (32, 50000), 
# log_file=tmp/nmt_attention_model_DE_s100_l2_u32/log_1574789515
  created train model with fresh parameters, time 0.15s
  created infer model with fresh parameters, time 0.08s
  # 1303
    src: Die Menschen beginnen sich zu fragen , warum ihre Führer sie zum Kämpfen bringen .
    ref: People begin to ask why their leaders are making them fight .
    nmt: 3600 Ricardo Ricardo NYC Macros hockey Cilento Cilento Cilento 8.3 8.3 8.3 8.3 Wonder hillsides answering Universidad Universidad Gli reflexion Taunus Avoid provokes Gli obstructing obstructing penguin penguin Albanian cheapest
  created eval model with fresh parameters, time 0.08s
  eval dev: perplexity 49999.83, time 29s, Tue Nov 26 14:32:25 2019.
  eval test: perplexity 49999.89, time 24s, Tue Nov 26 14:32:50 2019.
  created infer model with fresh parameters, time 0.06s
# Start step 0, lr 1, Tue Nov 26 14:32:50 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 1.87s wps 3.29K ppl 25843.76 gN 49.26 bleu 0.00, Tue Nov 26 14:35:57 2019
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l2_u32/translate.ckpt-100, time 0.24s
  # 107
    src: &quot; Bei der Kontrollgruppe der Umfragebeteiligten , die den Fragebogen nach Abschluss des aufklärenden Vortrags &quot; Adipositas ist kein Zufall &quot; ausfüllten , war in vielen Fällen eine deutliche Verschiebung der Präferenzen hin zu Grundsätzen der gesunden Ernährung zu verzeichnen &quot; , ist in den Ergebnissen des Projekts angeführt .
    ref: &quot; Many participants from the survey control group showed a significant shift of preferences towards healthy food when completing a questionnaire after the end of the Obesity is No Accident educational lecture , &quot; project results show .
    nmt: <unk> , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
  loaded eval model parameters from tmp/nmt_attention_model_DE_s100_l2_u32/translate.ckpt-100, time 0.04s
  eval dev: perplexity 52799.51, time 29s, Tue Nov 26 14:36:27 2019.
  eval test: perplexity 38248.75, time 24s, Tue Nov 26 14:36:52 2019.
  loaded infer model parameters from tmp/nmt_attention_model_DE_s100_l2_u32/translate.ckpt-100, time 0.03s
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l2_u32/output_dev
  done, num sentences 3003, num translations per input 1, time 57s, Tue Nov 26 14:37:49 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u32/hparams
# External evaluation, global step 100
  decoding to output tmp/nmt_attention_model_DE_s100_l2_u32/output_test
  done, num sentences 3000, num translations per input 1, time 51s, Tue Nov 26 14:38:42 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_DE_s100_l2_u32/hparams
# Final, step 100 lr 1 step-time 1.87s wps 3.29K ppl 25843.76 gN 49.26 dev ppl 52799.51, dev bleu 0.0, test ppl 38248.75, test bleu 0.0, Tue Nov 26 14:38:42 2019
# Done training!, time 352s, Tue Nov 26 14:38:42 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.07s
  # 1681
    src: Ihre Kritiker drängten Kagan , sich selbst von der Anhörung des Falles wegen Voreingenommenheit zurückzuziehen , da sie zu sehr in der Verteidigung des Gesetzes engagiert sei , um unvoreingenommen zu sein .
    ref: Her critics have pushed for Kagan to recuse herself from hearing the case , saying that she was too invested in defending the law then to be impartial now .
    nmt: Hackescher Hackescher revenues revenues liquidated liquidated untill Lunch Lunch Lunch IAS IAS swiftly swiftly swiftly Payerne Payerne Payerne Payerne Arrival Arrival risking risking risking risking hopelessness hopelessness hopelessness secluded insist insist insist ihre ihre beige beige beige ELAXY insist insist ihre beige beige RS232 reality reality reality reality reality reality alloys alloys alloys alloys availing availing availing availing boulders boulders von von von warm seafront Neuchâtel
  created eval model with fresh parameters, time 0.08s
  eval dev: perplexity 50000.79, time 30s, Tue Nov 26 14:39:13 2019.
  eval test: perplexity 50000.86, time 24s, Tue Nov 26 14:39:37 2019.
  created infer model with fresh parameters, time 0.07s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 1.87s wps 3.29K ppl 25843.76 gN 49.26 dev ppl 50000.79, dev bleu 0.0, test ppl 50000.86, test bleu 0.0, Tue Nov 26 14:39:39 2019
