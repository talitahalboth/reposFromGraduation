# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 7714229224650465011), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7078293537406505364)]
# Vocab file tmp/iwslt15/vocab.vi exists
# Vocab file tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_VI_s500_l2_u128/hparams
  saving hparams to tmp/nmt_attention_model_VI_s500_l2_u128/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_VI_s500_l2_u128/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=tmp/iwslt15/tst2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=2
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=2
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=500
  num_translations_per_input=1
  num_units=128
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_VI_s500_l2_u128
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=vi
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=tmp/iwslt15/vocab.vi
  src_vocab_size=7709
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=tmp/iwslt15/tst2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=tmp/iwslt15/vocab.en
  tgt_vocab_size=17191
  time_major=True
  train_prefix=tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=500, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 2, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 17191), 
# log_file=tmp/nmt_attention_model_VI_s500_l2_u128/log_1574884597
  created train model with fresh parameters, time 0.17s
  created infer model with fresh parameters, time 0.09s
  # 1116
    src: Điện thoại , máy tính , băng video , đĩa CD và nhiều thứ khác là những thiết bị chuyên dụng mà chúng ta tạo ra trong xã hội để làm việc với thông tin .
    ref: Telephony , computers , videotapes , CD-ROMs and so on are all our specialized mechanisms that we &apos;ve now built within our society for handling that information .
    nmt: du du pigeons fischeri synthesis vectors vectors extroverts extroverts extroverts extroverts extroverts extroverts vegetables dollar dollar dollar activities activities activities activities translated translated incense incense rocket rocket rocket rocket cooperative Lying musicians Lying ports approaches experimentation Bayes Bayes winner winner winner winner hoped demands demands demands affection affection affection affection affection staunch staunch staunch staunch staunch centered assume IMF IMF IMF IMF Khmer x-ray x-ray burned burned x-ray x-ray burned tract involving
  created eval model with fresh parameters, time 0.09s
  eval dev: perplexity 17193.02, time 6s, Wed Nov 27 16:56:44 2019.
  eval test: perplexity 17193.30, time 6s, Wed Nov 27 16:56:50 2019.
  created infer model with fresh parameters, time 0.08s
# Start step 0, lr 1, Wed Nov 27 16:56:51 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 1.07s wps 5.26K ppl 44556.62 gN 89.16 bleu 0.00, Wed Nov 27 16:58:38 2019
  step 200 lr 1 step-time 0.99s wps 5.63K ppl 1262.86 gN 17.62 bleu 0.00, Wed Nov 27 17:00:16 2019
  step 300 lr 1 step-time 1.00s wps 5.65K ppl 572.08 gN 8.22 bleu 0.00, Wed Nov 27 17:01:56 2019
  step 400 lr 1 step-time 1.00s wps 5.65K ppl 440.33 gN 6.95 bleu 0.00, Wed Nov 27 17:03:37 2019
  step 500 lr 1 step-time 1.01s wps 5.55K ppl 320.49 gN 5.64 bleu 0.00, Wed Nov 27 17:05:18 2019
  loaded infer model parameters from tmp/nmt_attention_model_VI_s500_l2_u128/translate.ckpt-500, time 2.18s
  # 1415
    src: Và nữa , tôi thậm chí còn đặt ngôi nhà lên đỉnh của hòn đảo làm cho nó trông Thuỵ Điển hoá hơn .
    ref: And yeah , I even turned the house on top of the island red to make it look more Swedish .
    nmt: And I can &apos;t &apos;t &apos;t &apos;t &apos;t a <unk> .
  loaded eval model parameters from tmp/nmt_attention_model_VI_s500_l2_u128/translate.ckpt-500, time 0.15s
  eval dev: perplexity 232.73, time 6s, Wed Nov 27 17:05:29 2019.
  eval test: perplexity 267.47, time 5s, Wed Nov 27 17:05:35 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s500_l2_u128/translate.ckpt-500, time 0.03s
# External evaluation, global step 500
  decoding to output tmp/nmt_attention_model_VI_s500_l2_u128/output_dev
  done, num sentences 1553, num translations per input 1, time 6s, Wed Nov 27 17:05:41 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_VI_s500_l2_u128/hparams
# External evaluation, global step 500
  decoding to output tmp/nmt_attention_model_VI_s500_l2_u128/output_test
  done, num sentences 1268, num translations per input 1, time 4s, Wed Nov 27 17:05:46 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_VI_s500_l2_u128/hparams
# Final, step 500 lr 1 step-time 1.01s wps 5.55K ppl 320.49 gN 5.64 dev ppl 232.73, dev bleu 0.0, test ppl 267.47, test bleu 0.0, Wed Nov 27 17:05:46 2019
# Done training!, time 535s, Wed Nov 27 17:05:46 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.07s
  # 1014
    src: Và điều thú vị là trang báo cáo này , giống như rất nhiều trang báo cáo công nghệ khác quen thuộc với chúng ta , là một loại đường thẳng nằm trên một bán đường cong log
    ref: And the interesting thing about it is that this slide , like so many technology slides that we &apos;re used to , is a sort of a straight line on a semi-log curve .
    nmt: retirement palpable audacity Hat cafeteria Mecca Mecca Mecca Mecca Mecca Mecca vicinity vicinity subsequent subsequent subsequent subsequent subsequent subsequent doorway insane insane insane route route route route route addressing addressing addressing asthmatic asthmatic asthmatic asthmatic asthmatic asthmatic asthmatic Byrne inexhaustible inexhaustible inexhaustible inexhaustible inexhaustible missile missile delicacy delicacy delicacy delicacy delicacy delicacy delicacy delicacy Fly Fly invent disappear disappear five five speeds speeds speeds just worms worms worms worms gymnosophist gymnosophist gymnosophist gymnosophist gymnosophist gymnosophist gymnosophist
  created eval model with fresh parameters, time 0.08s
  eval dev: perplexity 17189.81, time 5s, Wed Nov 27 17:05:52 2019.
  eval test: perplexity 17189.87, time 5s, Wed Nov 27 17:05:58 2019.
  created infer model with fresh parameters, time 0.07s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 1.01s wps 5.55K ppl 320.49 gN 5.64 dev ppl 17189.81, dev bleu 0.0, test ppl 17189.87, test bleu 0.0, Wed Nov 27 17:05:58 2019
