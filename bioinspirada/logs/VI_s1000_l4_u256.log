# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 14439689368729767588), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7602926233737996295)]
# Vocab file tmp/iwslt15/vocab.vi exists
# Vocab file tmp/iwslt15/vocab.en exists
  saving hparams to tmp/nmt_attention_model_VI_s1000_l4_u256/hparams
  saving hparams to tmp/nmt_attention_model_VI_s1000_l4_u256/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=tmp/nmt_attention_model_VI_s1000_l4_u256/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  coverage_penalty_weight=0.0
  decay_scheme=
  dev_prefix=tmp/iwslt15/tst2012
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=0
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=4
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=4
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=1000
  num_translations_per_input=1
  num_units=256
  optimizer=sgd
  out_dir=tmp/nmt_attention_model_VI_s1000_l4_u256
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=vi
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=tmp/iwslt15/vocab.vi
  src_vocab_size=7709
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=tmp/iwslt15/tst2013
  tgt=en
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=tmp/iwslt15/vocab.en
  tgt_vocab_size=17191
  time_major=True
  train_prefix=tmp/iwslt15/train
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=tmp/iwslt15/vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=1000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 4, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 2  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 3  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (7709, 256), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (17191, 256), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (256, 256), 
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (768, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (512, 1024), /device:GPU:0
  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (1024,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (512, 256), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), 
# log_file=tmp/nmt_attention_model_VI_s1000_l4_u256/log_1574895639
  created train model with fresh parameters, time 0.24s
  created infer model with fresh parameters, time 0.11s
  # 4
    src: Hãy tưởng tượng mảnh đầu tiên : một người đàn ông đốt cháy sự nghiệp cả đời mình .
    ref: Imagine the first piece : a man burning his life &apos;s work .
    nmt: excerpt ideologies ideologies ideologies ideologies ideologies ideologies optimize optimize optimize optimize optimize flames flames flames flames flames flames flames flames communism communism communism communism communism communism sacred sacred sacred sacred sacred sacred sacred sacred sacred sacred builder builder
  created eval model with fresh parameters, time 0.11s
  eval dev: perplexity 17224.12, time 11s, Wed Nov 27 20:00:51 2019.
  eval test: perplexity 17235.80, time 10s, Wed Nov 27 20:01:02 2019.
  created infer model with fresh parameters, time 0.11s
# Start step 0, lr 1, Wed Nov 27 20:01:02 2019
# Init train iterator, skipping 0 elements
  step 100 lr 1 step-time 2.17s wps 2.58K ppl 78245.62 gN 105.45 bleu 0.00, Wed Nov 27 20:04:39 2019
  step 200 lr 1 step-time 2.09s wps 2.66K ppl 10892.49 gN 51.63 bleu 0.00, Wed Nov 27 20:08:08 2019
  step 300 lr 1 step-time 2.11s wps 2.68K ppl 9742.52 gN 49.42 bleu 0.00, Wed Nov 27 20:11:39 2019
  step 400 lr 1 step-time 2.12s wps 2.68K ppl 5768.37 gN 44.56 bleu 0.00, Wed Nov 27 20:15:11 2019
  step 500 lr 1 step-time 2.09s wps 2.67K ppl 3961.33 gN 42.57 bleu 0.00, Wed Nov 27 20:18:40 2019
  step 600 lr 1 step-time 2.12s wps 2.68K ppl 3704.53 gN 42.75 bleu 0.00, Wed Nov 27 20:22:13 2019
  step 700 lr 1 step-time 2.09s wps 2.67K ppl 2989.22 gN 44.20 bleu 0.00, Wed Nov 27 20:25:42 2019
  step 800 lr 1 step-time 2.12s wps 2.67K ppl 2475.52 gN 41.26 bleu 0.00, Wed Nov 27 20:29:14 2019
  step 900 lr 1 step-time 2.09s wps 2.68K ppl 2676.04 gN 40.65 bleu 0.00, Wed Nov 27 20:32:43 2019
  step 1000 lr 1 step-time 2.09s wps 2.66K ppl 2879.38 gN 43.69 bleu 0.00, Wed Nov 27 20:36:12 2019
# Save eval, global step 1000
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l4_u256/translate.ckpt-1000, time 5.38s
  # 556
    src: Càng nhiều sự lựa chọn Càng nhiều người gửi tất cả tiền của họ vào những tài khoản thị trường tài chính
    ref: The more choices available , the more likely they were to put all their money in pure money market accounts .
    nmt: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
  loaded eval model parameters from tmp/nmt_attention_model_VI_s1000_l4_u256/translate.ckpt-1000, time 0.07s
  eval dev: perplexity 1588.82, time 11s, Wed Nov 27 20:36:34 2019.
  eval test: perplexity 1588.39, time 10s, Wed Nov 27 20:36:45 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l4_u256/translate.ckpt-1000, time 5.39s
  # 893
    src: Kết quả là bây giờ có hơn 10000 phiên bản bắt chước &quot; Friday &quot; trên YouTube .
    ref: And now there are 10,000 parodies of &quot; Friday &quot; on YouTube .
    nmt: And the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
  loaded eval model parameters from tmp/nmt_attention_model_VI_s1000_l4_u256/translate.ckpt-1000, time 0.06s
  eval dev: perplexity 1588.82, time 11s, Wed Nov 27 20:37:08 2019.
  eval test: perplexity 1588.39, time 10s, Wed Nov 27 20:37:18 2019.
  loaded infer model parameters from tmp/nmt_attention_model_VI_s1000_l4_u256/translate.ckpt-1000, time 0.06s
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l4_u256/output_dev
  done, num sentences 1553, num translations per input 1, time 69s, Wed Nov 27 20:38:28 2019.
  bleu dev: 0.0
  saving hparams to tmp/nmt_attention_model_VI_s1000_l4_u256/hparams
# External evaluation, global step 1000
  decoding to output tmp/nmt_attention_model_VI_s1000_l4_u256/output_test
  done, num sentences 1268, num translations per input 1, time 66s, Wed Nov 27 20:39:35 2019.
  bleu test: 0.0
  saving hparams to tmp/nmt_attention_model_VI_s1000_l4_u256/hparams
# Final, step 1000 lr 1 step-time 2.09s wps 2.66K ppl 2879.38 gN 43.69 dev ppl 1588.82, dev bleu 0.0, test ppl 1588.39, test bleu 0.0, Wed Nov 27 20:39:36 2019
# Done training!, time 2313s, Wed Nov 27 20:39:36 2019.
# Start evaluating saved best models.
  created infer model with fresh parameters, time 0.11s
  # 1268
    src: Đây cũng là một ví dụ điển hình khác về việc chính phủ triển khai trò chơi cộng đồng .
    ref: And it &apos;s also a great example of government getting in on the crowd-sourcing game .
    nmt: roller roller graphic Africa Africa graphic graphic graphic graphic Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo Tokyo arousal arousal arousal arousal arousal challenge challenge challenge challenge challenge challenge challenge mechanization theirs theirs theirs theirs theirs theirs
  created eval model with fresh parameters, time 0.12s
  eval dev: perplexity 17202.99, time 10s, Wed Nov 27 20:39:47 2019.
  eval test: perplexity 17234.29, time 10s, Wed Nov 27 20:39:58 2019.
  created infer model with fresh parameters, time 0.11s
  bleu dev: 0.0
  bleu test: 0.0
# Best bleu, step 0 lr 1 step-time 2.09s wps 2.66K ppl 2879.38 gN 43.69 dev ppl 17202.99, dev bleu 0.0, test ppl 17234.29, test bleu 0.0, Wed Nov 27 20:39:59 2019
